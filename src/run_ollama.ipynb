{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF3Y0q54B2S8",
        "outputId": "9ac25484-b3b5-49aa-9785-9ac3b6989e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ollama_model_id = \"deepseek-r1:1.5b\""
      ],
      "metadata": {
        "id": "Nbp2oJ-yCEYR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup bash -c \"OLLAMA_HOST=0.0.0.0:8000 OLLAMA_ORIGIN=* ollama serve\" &\n",
        "!sleep 5 && tail /content/nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsrWYDhLDEO7",
        "outputId": "d6959464-c099-4d9b-9443-7c6f555aaec0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "bash: line 1: 0.0.0.0:8000: command not found\n",
            "bash: line 1: 0.0.0.0:8000: command not found\n",
            "2025/04/22 23:48:47 routes.go:1231: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:8000 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-04-22T23:48:47.244Z level=INFO source=images.go:458 msg=\"total blobs: 5\"\n",
            "time=2025-04-22T23:48:47.244Z level=INFO source=images.go:465 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-04-22T23:48:47.244Z level=INFO source=routes.go:1298 msg=\"Listening on [::]:8000 (version 0.6.5)\"\n",
            "time=2025-04-22T23:48:47.244Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-04-22T23:48:47.260Z level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\n",
            "time=2025-04-22T23:48:47.260Z level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=cpu variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"334.6 GiB\" available=\"330.2 GiB\"\n",
            "Error: listen tcp 0.0.0.0:8000: bind: address already in use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull {ollama_model_id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ued3OwH4Dpxy",
        "outputId": "2cc5bbd2-63c7-4575-ee04-1c7f0cd2282f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup bash -c \"OLLAMA_HOST=0.0.0.0:8000 OLLAMA_ORIGIN=* ollama serve ollama run {ollama_model_id}\" &\n",
        "!sleep 5 && tail /content/nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PKd5m4VDxIK",
        "outputId": "62453c10-2e5c-45b4-d713-60a981872298"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "bash: line 1: 0.0.0.0:8000: command not found\n",
            "2025/04/22 23:48:47 routes.go:1231: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:8000 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-04-22T23:48:47.244Z level=INFO source=images.go:458 msg=\"total blobs: 5\"\n",
            "time=2025-04-22T23:48:47.244Z level=INFO source=images.go:465 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-04-22T23:48:47.244Z level=INFO source=routes.go:1298 msg=\"Listening on [::]:8000 (version 0.6.5)\"\n",
            "time=2025-04-22T23:48:47.244Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-04-22T23:48:47.260Z level=INFO source=gpu.go:377 msg=\"no compatible GPUs were discovered\"\n",
            "time=2025-04-22T23:48:47.260Z level=INFO source=types.go:130 msg=\"inference compute\" id=0 library=cpu variant=\"\" compute=\"\" driver=0.0 name=\"\" total=\"334.6 GiB\" available=\"330.2 GiB\"\n",
            "Error: listen tcp 0.0.0.0:8000: bind: address already in use\n",
            "Error: accepts 0 arg(s), received 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl http://localhost:8000/api/chat -d '{\n",
        "  \"model\": \"deepseek-r1:1.5b\",\n",
        "  \"messages\": [\n",
        "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
        "  ]\n",
        "}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljEcDflIEb_H",
        "outputId": "795b1022-d9be-4e3d-c87f-20f6a3364b3d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is interrupted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Y1TzgdIRzY",
        "outputId": "cde89166-d9dc-4341-d0f8-236eb82a2258"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.4-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok, conf"
      ],
      "metadata": {
        "id": "cs0ofGBWHirA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_auth= userdata.get('ngrok')\n",
        "conf.get_default().auth_token = ngrok_auth\n",
        "\n",
        "port = \"8000\"\n",
        "print(f\"Ngrok Auth: {ngrok_auth}\")\n",
        "\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A5tQW2-IWLc",
        "outputId": "22c5c668-ef8b-42b4-92af-78f1ebdcb853"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok Auth: 2w6dHMtj42R2ehL9JpzEdVWflAt_7HLpBeWosNnvLw93j4gsg\n",
            "https://b006-35-223-146-171.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl https://b006-35-223-146-171.ngrok-free.app/api/chat \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d '{\"model\":\"deepseek-r1:1.5b\",\"messages\":[{\"role\":\"user\",\"content\":\"why is the sky blue?\"}]}'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBr8rqaBIuv_",
        "outputId": "699aaf6c-7077-4aaf-b64b-388d9756d359"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-04-23T00:06:15+0000 lvl=warn msg=\"failed to open private leg\" id=18cc2f24e3d7 privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"h-full\" lang=\"en-US\" dir=\"ltr\">\n",
            "  <head>\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Regular-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-RegularItalic-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Medium-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Semibold-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-MediumItalic-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-Text.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-TextItalic.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-SemiBold.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-SemiBoldItalic.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <meta charset=\"utf-8\">\n",
            "    <meta name=\"author\" content=\"ngrok\">\n",
            "    <meta name=\"description\" content=\"ngrok is the fastest way to put anything on the internet with a single command.\">\n",
            "    <meta name=\"robots\" content=\"noindex, nofollow\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
            "    <link id=\"style\" rel=\"stylesheet\" href=\"https://cdn.ngrok.com/static/css/error.css\">\n",
            "    <noscript>Traffic was successfully tunneled to the ngrok agent, but the agent failed to establish a connection to the upstream web service at localhost:8000. (ERR_NGROK_8012)</noscript>\n",
            "    <script id=\"script\" src=\"https://cdn.ngrok.com/static/js/error.js\" type=\"text/javascript\"></script>\n",
            "  </head>\n",
            "  <body class=\"h-full\" id=\"ngrok\">\n",
            "    <div id=\"root\" data-payload=\"eyJhZGRyIjoibG9jYWxob3N0OjgwMDAiLCJjZG5CYXNlIjoiaHR0cHM6Ly9jZG4ubmdyb2suY29tLyIsImNvZGUiOiI4MDEyIiwiZXJyb3JUZXh0IjoiZGlhbCB0Y3AgMTI3LjAuMC4xOjgwMDA6IGNvbm5lY3Q6IGNvbm5lY3Rpb24gcmVmdXNlZCIsIm1lc3NhZ2UiOiJUcmFmZmljIHdhcyBzdWNjZXNzZnVsbHkgdHVubmVsZWQgdG8gdGhlIG5ncm9rIGFnZW50LCBidXQgdGhlIGFnZW50IGZhaWxlZCB0byBlc3RhYmxpc2ggYSBjb25uZWN0aW9uIHRvIHRoZSB1cHN0cmVhbSB3ZWIgc2VydmljZSBhdCBsb2NhbGhvc3Q6ODAwMC4iLCJzY2hlbWUiOiJodHRwIiwidGl0bGUiOiJCYWQgUmVxdWVzdCJ9\"></div>\n",
            "  </body>\n",
            "</html>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2894  100  2804  100    90  17052    547 --:--:-- --:--:-- --:--:-- 17646\n"
          ]
        }
      ]
    }
  ]
}